# Project 2 Generative Audio

Reid Brockmeier, rbrockmeier2@unl.edu

## Abstract

For this project, I want to train a model to sythesize my voice and use it to provide the transcript for a YouTube video. In order to accomplish this, I will need to figure out how exactly to train the data. Luckily, I have many hours of my voice along with transcripts on YouTube that I could utilize as training data. This could allow the trained model to have similar inflection to mine when I record a voiceover in a video. As for the video itself, I have a few different options of existing footage that I could make a video of, or I could use my previous project to generate a title to base a video on. I would prefer to make a new one, but my decision is reliant on how much time I will have. This project is continuing the theme of automating the creation of YouTube videos that I established in project 1.

## Model/Data

Briefly describe the files that are included with your repository:
- trained models
- training data (or link to training data)

## Code

Your code for generating your project:
- Python: generative_code.py
- Jupyter notebooks: generative_code.ipynb

## Results

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- `.wav` files or `.mp4`
- `.midi` files
- musical scores
- ... some other form

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference

References to any papers, techniques, repositories you used:
- Papers
- Repositories
- Blog posts
